{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:27:42.829915Z",
     "start_time": "2018-10-19T17:27:41.216005Z"
    }
   },
   "outputs": [],
   "source": [
    "from ai_base import List2CSV, CSV2List2\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from typing import List, Tuple\n",
    "import csv\n",
    "import numpy as np\n",
    "def doc2vec(trainData: List[List], testData: List[List], savePath = \"\",fname = \"doc2vec.model\")-> Tuple[np.array, np.array]:\n",
    "    documents = list(range(len(trainData)))\n",
    "    for i, doc in enumerate(trainData):\n",
    "        documents[i] = TaggedDocument(doc, [i])\n",
    "    vec_sz=50\n",
    "    model = Doc2Vec(documents, vector_size=vec_sz, window=2, min_count=1, workers=12)\n",
    "    model.save(fname)\n",
    "    trainX = np.zeros((len(trainData), vec_sz))\n",
    "    testX = np.zeros((len(testData), vec_sz))\n",
    "    for i, data in enumerate(trainData):\n",
    "        trainX[i] = model.infer_vector(data)\n",
    "    for i, data in enumerate(testData):\n",
    "        testX[i] = model.infer_vector(data)\n",
    "    List2CSV(savePath + \"doc2vec\" + \"TrainSet\" + str(vec_sz) + \"D\" + str(len(trainData)) + \"L.csv\", trainX)\n",
    "    List2CSV(savePath + \"doc2vec\" + \"TestSet\" + str(vec_sz) + \"D\" + str(len(testData)) + \"L.csv\", testX)\n",
    "    return (trainX, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:57:46.998502Z",
     "start_time": "2018-10-19T16:57:46.993379Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def _getTFIDF(fdata: List[List], word_dict: OrderedDict) -> np.array:\n",
    "    '''\n",
    "    获取TF-IDF矩阵，并将每个单词及出现次数存储到word_dict中\n",
    "    '''\n",
    "    #首先获取文章数和单词向量\n",
    "    #使用OrderedDict按单词出现的顺序生成单词列表\n",
    "    #相比于使用list，好处在于每次判断word是否已经加入单词向量是log(n)复杂度\n",
    "    #文章数\n",
    "    D = len(fdata)\n",
    "    if len(word_dict) is 0:\n",
    "        #训练集\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if not word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "                else:\n",
    "                    word_dict[word] += 1\n",
    "        word_dict[None] = 0\n",
    "    else:\n",
    "        #验证集和测试集，丢弃未出现的单词\n",
    "        word_dict = dict(zip(word_dict.keys(), [0 for _ in word_dict.values()]))\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word] += 1\n",
    "                else:\n",
    "                    word_dict[None] += 1\n",
    "    #word_vec是单词向量\n",
    "    word_vec = word_dict.keys()\n",
    "    #word_order的键值是当前单词的序号，在生成TF矩阵时会用到\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    #生成TF矩阵\n",
    "    TF = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            if word in word_order:\n",
    "                TF[i][word_order[word]] += 1\n",
    "            else:\n",
    "                TF[i][word_order[None]] += 1\n",
    "        #每个文章中单词出现次数归一化\n",
    "        TF[i] /= len(fdata[i])\n",
    "    #生成IDF矩阵\n",
    "    IDF = np.log2(D / (1 + np.array(list(word_dict.values()))))\n",
    "    #生成TF-IDF矩阵\n",
    "    TF_IDF = np.multiply(TF, IDF)\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:57:47.002989Z",
     "start_time": "2018-10-19T16:57:46.999696Z"
    }
   },
   "outputs": [],
   "source": [
    "def TFIDF(trainData: List[List], testData: List[List], savePath=\"\")-> Tuple[np.array, np.array]:\n",
    "    wdict = OrderedDict()\n",
    "    trainX = _getTFIDF(trainData, wdict)\n",
    "    testX = _getTFIDF(testData, wdict)\n",
    "    List2CSV(savePath + \"tfidf\" + \"TrainSet\" + str(len(wdict)) + \"D\" + str(len(trainData)) + \"L.csv\", trainX)\n",
    "    List2CSV(savePath + \"tfidf\" + \"TestSet\" + str(len(wdict)) + \"D\" + str(len(testData)) + \"L.csv\", testData)\n",
    "    return trainX, testX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:57:47.009381Z",
     "start_time": "2018-10-19T16:57:47.005099Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _getOneHot(fdata: List[List], word_dict: OrderedDict) -> np.array:\n",
    "    D = len(fdata)\n",
    "    if len(word_dict) is 0:\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if not word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "        word_dict[None] = 0\n",
    "    else:\n",
    "        word_dict = dict(zip(word_dict.keys(), [0 for _ in word_dict.values()]))\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "                else:\n",
    "                    word_dict[None] = 1\n",
    "    word_vec = word_dict.keys()\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    oneHot = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            if word in word_order:\n",
    "                oneHot[i][word_order[word]] = 1\n",
    "            else:\n",
    "                oneHot[i][word_order[None]] = 1\n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:57:47.013342Z",
     "start_time": "2018-10-19T16:57:47.010809Z"
    }
   },
   "outputs": [],
   "source": [
    "def OneHot(trainData: List[List], testData: List[List], savePath=\"\")-> Tuple[np.array, np.array]:\n",
    "    wdict = OrderedDict()\n",
    "    trainX = _getOneHot(trainData, wdict)\n",
    "    testX = _getOneHot(testData, wdict)\n",
    "    List2CSV(savePath + \"onehot\" + \"TrainSet\" + str(len(wdict)) + \"D\" + str(len(trainData)) + \"L.csv\", trainX)\n",
    "    List2CSV(savePath + \"onehot\" + \"TestSet\" + str(len(wdict)) + \"D\" + str(len(testData)) + \"L.csv\", testData)\n",
    "    return trainX, testX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:57:48.316390Z",
     "start_time": "2018-10-19T16:57:47.015323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import csv\n",
    "# ret = []\n",
    "# with open('data/2/clean/trainDataclean.csv', 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         ret.append(list(row))\n",
    "trainData = CSV2List2('data/2/clean/trainDataclean.csv')\n",
    "testData = CSV2List2('data/2/clean/testDataclean.csv')\n",
    "trainData = [[w.lower() for w in row] for row in trainData]\n",
    "trainData = [[w.lower() for w in row] for row in testData]\n",
    "len(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:58:01.674990Z",
     "start_time": "2018-10-19T16:57:48.317665Z"
    }
   },
   "outputs": [],
   "source": [
    "trainX, testX = doc2vec(trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T07:56:19.042563Z",
     "start_time": "2018-10-19T07:55:51.432704Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "trainX2, testX2 = TFIDF(trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T07:57:11.978603Z",
     "start_time": "2018-10-19T07:56:47.490941Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "trainX3, testX3 = OneHot(trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:58:01.734532Z",
     "start_time": "2018-10-19T16:58:01.676211Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e8a25c97f9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainX3' is not defined"
     ]
    }
   ],
   "source": [
    "sum(trainX3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:58:19.521286Z",
     "start_time": "2018-10-19T16:58:19.518902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T03:03:54.621796Z",
     "start_time": "2018-10-20T03:03:53.931646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import KNN\n",
    "from ai_base import CSV2List2, List2CSV\n",
    "import numpy as np\n",
    "trainX = np.float_(CSV2List2('doc2vecTrainSet50D24000L.csv'))\n",
    "trainYData = CSV2List2('data/2/clean/trainLabel.txt')\n",
    "testData = np.float_(CSV2List2('doc2vecTestSet50D6000L.csv'))\n",
    "len(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T03:03:54.753935Z",
     "start_time": "2018-10-20T03:03:54.750764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29104736,  0.0768804 , -0.16329062,  0.15835629,  0.08793257,\n",
       "        0.20293267, -0.30032602, -0.08988713,  0.00514956,  0.18893704,\n",
       "        0.33035704,  0.11677902,  0.0503263 , -0.15362824,  0.15530102,\n",
       "        0.05387696, -0.54698908,  0.31815219,  0.11182838, -0.21462551,\n",
       "       -0.18355298,  0.2321436 ,  0.23971845, -0.05463126, -0.16444041,\n",
       "        0.02434071,  0.39980423,  0.04631198, -0.2933962 ,  0.36511028,\n",
       "        0.26598665,  0.2677407 ,  0.11230476,  0.07896607, -0.32979521,\n",
       "       -0.02836336, -0.15179099, -0.16036966,  0.17397907, -0.28041485,\n",
       "       -0.10498746, -0.23027968,  0.10070526,  0.12655641,  0.1419463 ,\n",
       "       -0.26576224,  0.53412449,  0.12394352,  0.05899602,  0.10648291])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T03:03:55.205093Z",
     "start_time": "2018-10-20T03:03:55.195358Z"
    }
   },
   "outputs": [],
   "source": [
    "KNNtrainX = np.array(trainX[0:20000])\n",
    "KNNvaildX = np.array(trainX[20000:])\n",
    "\n",
    "\n",
    "KNNtrainY = np.array(np.float_(trainYData[0:20000]))\n",
    "KNNvaildY = np.array(np.float_(trainYData[20000:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T03:03:59.023608Z",
     "start_time": "2018-10-20T03:03:59.018080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNtrainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:05:41.277096Z",
     "start_time": "2018-10-19T17:05:40.997430Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from ai_base import CSV2List2, List2CSV\n",
    "List2CSV(data=KNNtrainX, filen='1')\n",
    "List2CSV(data=KNNvaildX, filen='2')\n",
    "List2CSV(data=KNNtrainY, filen='3')\n",
    "List2CSV(data=KNNvaildY, filen='4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:19:03.091945Z",
     "start_time": "2018-10-19T17:19:02.949698Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from ai_base import CSV2List2, List2CSV\n",
    "KNNtrainX = np.array(np.float_(CSV2List2('1')))\n",
    "KNNvaildX = np.array(np.float_(CSV2List2('2')))\n",
    "KNNtrainY = np.array(np.float_(CSV2List2('3')))\n",
    "KNNvaildY = np.array(np.float_(CSV2List2('4')))\n",
    "\n",
    "KNNtrainX = KNNtrainX[0:2000]\n",
    "KNNvaildX = KNNvaildX[0:1000]\n",
    "KNNtrainY = KNNtrainY[0:2000]\n",
    "KNNvaildY = KNNvaildY[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T03:04:00.665410Z",
     "start_time": "2018-10-20T03:04:00.660174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNvaildY.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T03:04:19.203405Z",
     "start_time": "2018-10-20T03:04:19.191287Z"
    }
   },
   "outputs": [],
   "source": [
    "import ai_base\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Callable\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from scipy.stats.stats import pearsonr\n",
    "from time import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 一范数\n",
    "Dis1 = lambda v1, v2: np.linalg.norm(v1 - v2, 1)\n",
    "# 二范数\n",
    "Dis2 = lambda v1, v2: np.linalg.norm(v1 - v2, 2)\n",
    "# 无穷范数\n",
    "DisInf = lambda v1, v2: np.linalg.norm(v1 - v2, np.inf)\n",
    "# 余弦距离（1-余弦相关度）\n",
    "def DisCosine(v1, v2):\n",
    "    t1 = np.dot(v1,v2)\n",
    "    t2 = np.linalg.norm(v1)\n",
    "    t3 = np.linalg.norm(v2)\n",
    "    ret = 1 - t1 / (t2*t3)\n",
    "    return ret\n",
    "    \n",
    "def DisInvNormAvg(distances: np.array, Y: np.array) -> np.array:\n",
    "    '''\n",
    "    按照归一化的距离倒数加权求和，返回均值\n",
    "    '''\n",
    "    # 如果训练集中有向量距离和待预测向量完全一致（距离为0）\n",
    "    for idx, dis in enumerate(distances):\n",
    "        if np.isclose(dis, 0):\n",
    "            # 则直接返回该训练集向量对应的Y\n",
    "            return Y[idx]\n",
    "    # 求距离的倒数\n",
    "    distances = np.array(1.0) / distances\n",
    "    # 归一化\n",
    "    s = np.sum(distances)\n",
    "    distances = distances / s\n",
    "    # 分别作为权值乘以K个最邻近的训练集向量对应的Y\n",
    "    tmp = np.diag(distances) @ Y  \n",
    "    # 加权后Y的个分量求和\n",
    "    if len(tmp.shape) is 1:\n",
    "        return tmp\n",
    "    else:\n",
    "        return np.sum(tmp,  axis = (0))\n",
    "\n",
    "def classifyParseY(ydata: List[str], n: int)->np.array:\n",
    "    '''\n",
    "    Convert Y data from raw string list to matrix consisted of Y vectors\n",
    "    e.g.\n",
    "    [\"anger\", \"disgust\", ..., \"surprise\"] -> \n",
    "    |1, 0, 0, 0, 0, 0|\n",
    "    |0, 1, 0, 0, 0, 0|\n",
    "    |0, 0, ...,  0, 0|\n",
    "    |0, 0, 0, 0, 1, 0|\n",
    "    |0, 0, 0, 0, 0, 1|\n",
    "    '''\n",
    "    D = len(ydata)\n",
    "    \n",
    "    #fast hash ydata from strings [\"anger\", \"disgust\", ...] to [1, 2, ...]^T\n",
    "    #ydata = np.array(ydata).reshape((-1,1))\n",
    "    \n",
    "    '''\n",
    "    ymat is the column-wise repeat of ydata.\n",
    "    e.g.\n",
    "    |0|      |0, 0, 0, 0, 0, 0|\n",
    "    |1|   -> |1, 1, 1, 1, 1, 1|\n",
    "    ...      |................|\n",
    "    |5|      |5, 5, 5, 5, 5, 5|\n",
    "    ydata -> ymat\n",
    "    '''\n",
    "    ymat  = np.tile(ydata, (1, n))\n",
    "    \n",
    "    '''\n",
    "    ycmp is a matrix of which each row is [0, 1, 2, 3, 4, 5]\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    |................|\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    '''\n",
    "    ycmp  = np.tile(np.array(range(n)), (D, 1))\n",
    "    return np.int_(np.equal(ymat, ycmp))\n",
    "\n",
    "def KNN(trainSet: Tuple[np.array, np.array],\n",
    "        testVec: np.array,\n",
    "        DisFunc: Callable[[np.array, np.array], float],\n",
    "        K: int,\n",
    "        WeightFunc: Callable[[np.array, np.array], float]) -> np.array: \n",
    "    '''\n",
    "    一个通用的KNN接口\n",
    "    trainSet: 二元元组，第一个元素是训练集的X，第二个是Y\n",
    "    testVec: 待预测向量\n",
    "    DisFunc: 距离函数\n",
    "    K: K值\n",
    "    WeightFunc: 依据第一个参数list<距离>,对第二个参数list<Y值>进行加权，返回预测值\n",
    "    '''\n",
    "    #对于多个要预测的值，逐一预测\n",
    "#     if len(testVec.shape) > 1:\n",
    "#         n = len(testVec)\n",
    "#         ret = list(range(n))\n",
    "#         for i in tnrange(n):\n",
    "#             ret[i] = KNN(trainSet, testVec[i], DisFunc, K, WeightFunc)\n",
    "#         return np.array(ret)\n",
    "#     else:\n",
    "        #测量待预测向量到训练集中每个向量的距离\n",
    "        #distances是一个list<tuple(index, distance)>\n",
    "        \n",
    "#     distances = list(enumerate(map(lambda trainVec: DisFunc(trainVec, testVec), trainSet[0])))\n",
    "    \n",
    "    test_sum = np.sum(np.square(testVec), axis=1)  # num_test x 1\n",
    "    train_sum = np.sum(np.square(trainSet[0]), axis=1)  # num_train x 1\n",
    "    inner_product = np.dot(testVec, trainSet[0].T)  # num_test x num_train\n",
    "    dists = np.sqrt(-2 * inner_product + test_sum.reshape(-1, 1) + train_sum)  # broadcast\n",
    "    n = len(testVec)\n",
    "    ret = list(range(n))\n",
    "    for i in tnrange(n):\n",
    "        distances = list(enumerate(dists[i]))\n",
    "        #依据距离从小到大排序\n",
    "        distances.sort(key=lambda t: t[1])\n",
    "        #获取最临近的K个训练样本的下标和对应的距离，输出值\n",
    "        tmp = list(zip(*distances[:K]))\n",
    "        kNearIdx = list(tmp[0])\n",
    "        kNearDis = list(tmp[1])\n",
    "        kNearY   = trainSet[1][kNearIdx, :]\n",
    "        #对输出值根据距离加权作为预测输出\n",
    "        ret[i] = WeightFunc(kNearDis, kNearY)\n",
    "\n",
    "def get_regress(predictY, vaildY):\n",
    "    r = [pearsonr(predictY[:, i], vaildY[:, i])[0] for i in range(vaildY.shape[1])]\n",
    "    average = np.average(r)\n",
    "    print(\"Correlation Coefficient: \", average)\n",
    "    return average\n",
    "\n",
    "def get_classify(predictY, vaildY):\n",
    "    classifyY = np.zeros_like(predictY)\n",
    "    for i, row in enumerate(predictY):\n",
    "        m = 0\n",
    "        idx = 0\n",
    "        for j, v in enumerate(row):\n",
    "            if v > m:\n",
    "                m = v\n",
    "                idx = j\n",
    "        classifyY[i][idx] = 1\n",
    "    ret = np.sum(np.logical_and(classifyY, vaildY)) / vaildY.shape[0]\n",
    "    print(\"Classification Accuracy: \", ret)\n",
    "    return ret\n",
    "\n",
    "def autoTrain(trainSet: Tuple, vaildSet:Tuple):\n",
    "    trainX, trainY = trainSet\n",
    "    vaildX, vaildY = vaildSet\n",
    "    print(\"Start training...\")\n",
    "    t = time()\n",
    "    K_val = range(8, 14)\n",
    "#     DisFuncs = {\"Dis1\": Dis1, \"Dis2\": Dis2, \"DisInf\": DisInf, \"DisCosine\": DisCosine}\n",
    "    DisFuncs = {\"Dis2\": Dis2}\n",
    "    results_reg = OrderedDict()\n",
    "    results_cla = OrderedDict()\n",
    "    for K in K_val:\n",
    "        for dfname, DisFunc in DisFuncs.items():\n",
    "            predictY = KNN((trainX,trainY), vaildX, DisFunc, K, DisInvNormAvg)\n",
    "            cla_ret = get_classify(predictY, vaildY)\n",
    "            reg_ret = get_regress(predictY, vaildY)\n",
    "            results_reg[(K, dfname)] = cla_ret\n",
    "            results_cla[(K, dfname)] = reg_ret\n",
    "            print(K, dfname, \":\", cla_ret, reg_ret)\n",
    "    print(\"{} groups of argument tested, spent {}s\".format(len(K_val) * len(DisFuncs), time() - t))\n",
    "    return results\n",
    "\n",
    "def vaild(trainSet: Tuple, vaildSet: Tuple, K, DisFunc):\n",
    "    trainX, trainY = trainSet\n",
    "    vaildX, vaildY = vaildSet\n",
    "    predictY = KNN(trainSet,vaildX,DisFunc,K,DisInvNormAvg)\n",
    "    cla_ret = get_classify(predictY, vaildY)\n",
    "    reg_ret = get_regress(predictY, vaildY)\n",
    "    print(pfname, K, dfname, \":\", cla_ret, reg_ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T03:04:56.199893Z",
     "start_time": "2018-10-20T03:04:20.147524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac95e5269d4243f8ab60bf5c0d4f2a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-537db1151f0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautoTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNNtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNNtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKNNvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNNvaildY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-4aa33446f33a>\u001b[0m in \u001b[0;36mautoTrain\u001b[0;34m(trainSet, vaildSet)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisFunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDisFuncs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mpredictY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisInvNormAvg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mcla_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mreg_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_regress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mresults_reg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcla_ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4aa33446f33a>\u001b[0m in \u001b[0;36mget_classify\u001b[0;34m(predictY, vaildY)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "autoTrain((KNNtrainX, KNNtrainY), (KNNvaildX, KNNvaildY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
