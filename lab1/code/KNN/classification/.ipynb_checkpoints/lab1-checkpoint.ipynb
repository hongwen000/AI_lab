{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from typing import Callable\n",
    "from typing import Any\n",
    "from typing import Dict, Tuple, List\n",
    "import cProfile\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filen: str) -> List[List]:\n",
    "    '''\n",
    "    读取文件内容\n",
    "    由于首先需要获取文章数量和单词向量长度，才能计算TF矩阵\n",
    "    因此要对文本内容进行两次遍历，为了避免两次读取磁盘文件，故先将文本内容保存到内存中的一个list\n",
    "    '''\n",
    "    fdata = []\n",
    "    with open(filen) as fd:\n",
    "        reader = csv.reader(fd, delimiter=' ')\n",
    "        fdata = [list(row) for row in reader]\n",
    "    return fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(fdata: List[List]) -> np.array:\n",
    "    '''\n",
    "    获取TF-IDF矩阵\n",
    "    '''\n",
    "    #首先获取文章数和单词向量\n",
    "    #使用OrderedDict按单词出现的顺序生成单词列表\n",
    "    #相比于使用list，好处在于每次判断word是否已经加入单词向量是log(n)复杂度\n",
    "    word_dict = OrderedDict() \n",
    "    #文章数\n",
    "    D = 0\n",
    "    for row in fdata:\n",
    "        D += 1\n",
    "        for word in row:\n",
    "            if not word in word_dict:\n",
    "                word_dict[word] = 1\n",
    "            else:\n",
    "                word_dict[word] += 1\n",
    "    #word_vec是单词向量\n",
    "    word_vec = word_dict.keys()\n",
    "    #word_order的键值是当前单词的序号，在生成TF矩阵时会用到\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    #生成TF矩阵\n",
    "    TF = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            TF[i][word_order[word]] += 1\n",
    "        #每个文章中单词出现次数归一化\n",
    "        TF[i] /= len(fdata[i])\n",
    "    #生成IDF矩阵\n",
    "    IDF = np.log(D / (1 + np.array(list(word_dict.values()))))\n",
    "\n",
    "    #生成TF-IDF矩阵\n",
    "    TF_IDF = np.multiply(TF, IDF)\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.20273255,  0.        ],\n",
       "       [ 0.        , -0.20273255,  0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTFIDF([[\"b\", \"c\"], [\"a\", \"c\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n"
     ]
    }
   ],
   "source": [
    "semval = readFile('lab1_data/semeval_sliced.txt')\n",
    "ret = getTFIDF(semval)\n",
    "#ret = ret.tolist()\n",
    "#rt = []\n",
    "#for row in ret:\n",
    "#    rr = []\n",
    "#    for w in row:\n",
    "#        if row != 0:\n",
    "#           rr.append(w)\n",
    "#    rt.append(rr)\n",
    "#ret = np.array(rt)\n",
    "##print(ret)\n",
    "np.savetxt(\"15323032_LiXinrui_TFIDF.txt\", ret, delimiter=\" \", fmt=\"%6f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_getTFIDF(fdata: List[List], word_dict: OrderedDict) -> np.array:\n",
    "    '''\n",
    "    获取TF-IDF矩阵，并将每个单词及出现次数存储到word_dict中\n",
    "    '''\n",
    "    #首先获取文章数和单词向量\n",
    "    #使用OrderedDict按单词出现的顺序生成单词列表\n",
    "    #相比于使用list，好处在于每次判断word是否已经加入单词向量是log(n)复杂度\n",
    "    #文章数\n",
    "    D = len(fdata)\n",
    "    if len(word_dict) is 0:\n",
    "        #训练集\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if not word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "                else:\n",
    "                    word_dict[word] += 1\n",
    "        word_dict[None] = 0\n",
    "    else:\n",
    "        #验证集和测试集，丢弃未出现的单词\n",
    "        word_dict = dict(zip(word_dict.keys(), [0 for _ in word_dict.values()]))\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word] += 1\n",
    "                else:\n",
    "                    word_dict[None] += 1\n",
    "    #word_vec是单词向量\n",
    "    word_vec = word_dict.keys()\n",
    "    #word_order的键值是当前单词的序号，在生成TF矩阵时会用到\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    #生成TF矩阵\n",
    "    TF = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            if word in word_order:\n",
    "                TF[i][word_order[word]] += 1\n",
    "            else:\n",
    "                TF[i][word_order[None]] += 1\n",
    "        #每个文章中单词出现次数归一化\n",
    "        TF[i] /= len(fdata[i])\n",
    "    #生成IDF矩阵\n",
    "    IDF = np.log2(D / (1 + np.array(list(word_dict.values()))))\n",
    "    #生成TF-IDF矩阵\n",
    "    TF_IDF = np.multiply(TF, IDF)\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. ,  0. ,  0. ],\n",
       "       [-0.5,  0. ,  0. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdict = OrderedDict()\n",
    "KNN_getTFIDF([['a','a'],['c','a']], wdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneHot(fdata: List[List], word_dict: OrderedDict) -> np.array:\n",
    "    D = len(fdata)\n",
    "    if len(word_dict) is 0:\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if not word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "        word_dict[None] = 0\n",
    "    else:\n",
    "        word_dict = dict(zip(word_dict.keys(), [0 for _ in word_dict.values()]))\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "                else:\n",
    "                    word_dict[None] = 1\n",
    "    word_vec = word_dict.keys()\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    oneHot = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            if word in word_order:\n",
    "                oneHot[i][word_order[word]] = 1\n",
    "            else:\n",
    "                oneHot[i][word_order[None]] = 1\n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdict = OrderedDict()\n",
    "getOneHot([['a','a'],['c','a']], wdict)\n",
    "getOneHot([['d','d']], wdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisN(vec1: np.array, vec2: np.array, N: Any) -> float:\n",
    "    '''\n",
    "    计算N-norm\n",
    "    '''\n",
    "    if(N < 1):\n",
    "        raise ValueError(\"norm should be a positive integer or np.inf\")\n",
    "    if np.isinf(N):\n",
    "        return np.max(np.fabs(vec1 - vec2))\n",
    "    else:\n",
    "        return np.power(np.sum(np.power(vec1 - vec2, N)), 1.0/N)\n",
    "\n",
    "#Dis2 = lambda v1, v2: DisN(v1, v2, 2)\n",
    "# 一范数\n",
    "Dis1 = lambda v1, v2: np.linalg.norm(v1 - v2, 1)\n",
    "# 二范数\n",
    "Dis2 = lambda v1, v2: np.linalg.norm(v1 - v2, 2)\n",
    "# 无穷范数\n",
    "DisInf = lambda v1, v2: np.linalg.norm(v1 - v2, np.inf)\n",
    "# 余弦距离（1-余弦相关度）\n",
    "def DisCosine(v1, v2):\n",
    "    t1 = np.dot(v1,v2)\n",
    "    t2 = np.linalg.norm(v1)\n",
    "    t3 = np.linalg.norm(v2)\n",
    "    ret = 1 - t1 / (t2*t3)\n",
    "    return ret\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisCosine([0,1],[0,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisInvNormAvg(distances: np.array, Y: np.array) -> np.array:\n",
    "    '''\n",
    "    按照归一化的距离倒数加权求和，返回均值\n",
    "    '''\n",
    "    # 如果训练集中有向量距离和待预测向量完全一致（距离为0）\n",
    "    for idx, dis in enumerate(distances):\n",
    "        if np.isclose(dis, 0):\n",
    "            # 则直接返回该训练集向量对应的Y\n",
    "            return Y[idx]\n",
    "    # 求距离的倒数\n",
    "    distances = np.array(1.0) / distances\n",
    "    # 归一化\n",
    "    s = np.sum(distances)\n",
    "    distances = distances / s\n",
    "    # 分别作为权值乘以K个最邻近的训练集向量对应的Y\n",
    "    tmp = np.diag(distances) @ Y  \n",
    "    # 加权后Y的个分量求和\n",
    "    if len(tmp.shape) is 1:\n",
    "        return tmp\n",
    "    else:\n",
    "        return np.sum(tmp,  axis = (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(trainSet: Tuple[np.array, np.array],\n",
    "        testVec: np.array,\n",
    "        DisFunc: Callable[[np.array, np.array], float],\n",
    "        K: int,\n",
    "        WeightFunc: Callable[[np.array, np.array], float]) -> np.array: \n",
    "    '''\n",
    "    一个通用的KNN接口\n",
    "    trainSet: 二元元组，第一个元素是训练集的X，第二个是Y\n",
    "    testVec: 待预测向量\n",
    "    DisFunc: 距离函数\n",
    "    K: K值\n",
    "    WeightFunc: 依据第一个参数list<距离>,对第二个参数list<Y值>进行加权，返回预测值\n",
    "    '''\n",
    "    #对于多个要预测的值，逐一预测\n",
    "    if len(testVec.shape) > 1:\n",
    "        return np.array([KNN(trainSet, vec, DisFunc, K, WeightFunc) for vec in testVec])\n",
    "    else:\n",
    "        #测量待预测向量到训练集中每个向量的距离\n",
    "        #distances是一个list<tuple(index, distance)>\n",
    "        distances = list(enumerate(map(lambda trainVec: DisFunc(trainVec, testVec), trainSet[0])))\n",
    "        #依据距离从小到大排序\n",
    "        distances.sort(key=lambda t: t[1])\n",
    "        #获取最临近的K个训练样本的下标和对应的距离，输出值\n",
    "        tmp = list(zip(*distances[:K]))\n",
    "        kNearIdx = list(tmp[0])\n",
    "        kNearDis = list(tmp[1])\n",
    "        kNearY   = trainSet[1][kNearIdx, :]\n",
    "        #对输出值根据距离加权作为预测输出\n",
    "        return WeightFunc(kNearDis, kNearY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestCast1():\n",
    "    trainX = np.array([[10,2],[2,3],[3,5]])\n",
    "    trainY = np.array([[1,1,1], [2,2,3], [3,3,5]])\n",
    "    vaildX = np.array([[3,3]])\n",
    "    return trainX, trainY, vaildX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 3.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX, trainY, vaildX = TestCast1()\n",
    "KNN((trainX, trainY), vaildX, Dis2, 1, DisInvNormAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestCast2():\n",
    "    xfilen = 'lab1_data/X.txt'\n",
    "    yfilen = 'lab1_data/Y.txt'\n",
    "    xdata = readFile(xfilen)\n",
    "    ydata = readFile(yfilen)\n",
    "    x_set = getTFIDF(xdata)\n",
    "    y_set = np.array([list(map(float, row)) for row in ydata])\n",
    "    DIVIDE_RATE = 0.75\n",
    "    train_D = int(np.ceil(x_set.shape[0] * DIVIDE_RATE))\n",
    "    trainX = x_set[0:train_D, :]\n",
    "    vaildX = x_set[train_D:, :]\n",
    "    trainY = y_set[0:train_D, :]\n",
    "    vaildY = y_set[train_D:, :]\n",
    "    return trainX, trainY, vaildX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(trainX, trainY, vaildX):\n",
    "    t = time()\n",
    "    cProfile.run('KNN((trainX, trainY), vaildX,Dis2,2,DisInvNormAvg)')\n",
    "    print(time()-t) \n",
    "#trainX, trainY, vaildX = TestCast2()\n",
    "#Test(trainX, trainY, vaildX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyReadFile(filen: str) -> Tuple[List[str], List[str]]:\n",
    "    with open(filen) as fd:\n",
    "        reader = csv.reader(fd, delimiter=',')\n",
    "        train_data = [list(row) for row in reader]\n",
    "        train_data = train_data[1:]\n",
    "        tmp = list(zip(*train_data))\n",
    "        xdata = [row.split() for row in list(tmp[0])]\n",
    "        ydata = list(tmp[1])\n",
    "        return xdata, ydata\n",
    "\n",
    "def vectorizeData(xdata, ydata, xVecFunc, yVecFunc):\n",
    "    return xVecFunc(xdata), yVecFunc(ydata)\n",
    "\n",
    "def fastHashY(s: str) -> int:\n",
    "    if s[0] is 'a': return 0\n",
    "    if s[0] is 'd': return 1\n",
    "    if s[0] is 'f': return 2\n",
    "    if s[0] is 'j': return 3\n",
    "    if s[1] is 'a': return 4\n",
    "    return 5\n",
    "\n",
    "def classifyParseY(ydata: List[str])->np.array:\n",
    "    '''\n",
    "    Convert Y data from raw string list to matrix consisted of Y vectors\n",
    "    e.g.\n",
    "    [\"anger\", \"disgust\", ..., \"surprise\"] -> \n",
    "    |1, 0, 0, 0, 0, 0|\n",
    "    |0, 1, 0, 0, 0, 0|\n",
    "    |0, 0, ...,  0, 0|\n",
    "    |0, 0, 0, 0, 1, 0|\n",
    "    |0, 0, 0, 0, 0, 1|\n",
    "    '''\n",
    "    D = len(ydata)\n",
    "    \n",
    "    #fast hash ydata from strings [\"anger\", \"disgust\", ...] to [1, 2, ...]^T\n",
    "    ydata = np.array(list(map(fastHashY, ydata))).reshape((-1,1))\n",
    "    \n",
    "    '''\n",
    "    ymat is the column-wise repeat of ydata.\n",
    "    e.g.\n",
    "    |0|      |0, 0, 0, 0, 0, 0|\n",
    "    |1|   -> |1, 1, 1, 1, 1, 1|\n",
    "    ...      |................|\n",
    "    |5|      |5, 5, 5, 5, 5, 5|\n",
    "    ydata -> ymat\n",
    "    '''\n",
    "    ymat  = np.tile(ydata, (1, 6))\n",
    "    \n",
    "    '''\n",
    "    ycmp is a matrix of which each row is [0, 1, 2, 3, 4, 5]\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    |................|\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    '''\n",
    "    ycmp  = np.tile(np.array(range(6)), (D, 1))\n",
    "    return np.int_(np.equal(ymat, ycmp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_classify(trainX, trainY, vaildX, vaildY, knnFunc):\n",
    "    predictY = knnFunc(trainX, trainY, vaildX)\n",
    "    classifyY = np.zeros_like(predictY)\n",
    "    for i, row in enumerate(predictY):\n",
    "        m = 0\n",
    "        idx = 0\n",
    "        for j, v in enumerate(row):\n",
    "            if v > m:\n",
    "                m = v\n",
    "                idx = j\n",
    "        classifyY[i][idx] = 1\n",
    "    #print(\"Predicted Y\")\n",
    "    #print(classifyY)\n",
    "    #print(\"Correct Y\")\n",
    "    #print(vaildY)\n",
    "    ret = np.sum(np.logical_and(classifyY, vaildY)) / vaildX.shape[0]\n",
    "    print(\"Classification Accuracy: \", ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressReadFile(filen: str) -> Tuple[List[str], List[str]]:\n",
    "    with open(filen) as fd:\n",
    "        reader = csv.reader(fd, delimiter=',')\n",
    "        train_data = [list(row) for row in reader]\n",
    "        train_data = train_data[1:]\n",
    "        xdata = [row[0].split() for row in train_data]\n",
    "        ydata = [[row[i] for i in range(1, 7)] for row in train_data]\n",
    "        return xdata, ydata\n",
    "\n",
    "def regressParseY(ydata: List[List[str]]) -> np.array:\n",
    "    if ydata[0][0] is '?':\n",
    "        return np.zeros_like(ydata)\n",
    "    return np.float_(np.array(ydata))\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "#def pearsonr(X, Y):\n",
    "#\n",
    "#    X_bar = np.average(X)\n",
    "#    Y_bar = np.average(Y)\n",
    "#    X = np.subtract(X, X_bar)\n",
    "#    Y = np.subtract(Y, Y_bar)\n",
    "#    t1 = np.sum(np.dot(X, Y))\n",
    "#    t2 = np.sum(np.power(X,2))\n",
    "#    t3 = np.sum(np.power(Y,2))\n",
    "#\n",
    "#    ret = t1 / np.power(t2 * t3, 0.5)\n",
    "#    return ret\n",
    "def do_regress(trainX, trainY, vaildX, vaildY, knnFunc, save = False):\n",
    "    predictY = knnFunc(trainX, trainY, vaildX)\n",
    "    if save:\n",
    "        np.savetxt(\"regress.csv\", predictY, delimiter=\",\", fmt=\"%4f\")\n",
    "    r = [pearsonr(predictY[:, i], vaildY[:, i])[0] for i in range(6)]\n",
    "    average = np.average(r)\n",
    "    print(\"Correlation Coefficient: \", average)\n",
    "    return average\n",
    "    \n",
    "    \n",
    "train_filen = 'lab1_data/regression_dataset/train_set.csv'\n",
    "vaild_filen = 'lab1_data/regression_dataset/validation_set.csv'\n",
    "test_filen  = 'lab1_data/regression_dataset/test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_filen = 'lab1_data/classification_dataset/train_set.csv'\n",
    "#vaild_filen = 'lab1_data/classification_dataset/validation_set.csv'\n",
    "#test_filen  = 'lab1_data/classification_dataset/test_set.csv'\n",
    "#\n",
    "#trainX_data, trainY_data = classifyReadFile(train_filen)\n",
    "#vaildX_data, vaildY_data = classifyReadFile(vaild_filen)\n",
    "#\n",
    "#ParseFuncs = {\"OneHot\": getOneHot, \"TI-IDF\": KNN_getTFIDF}\n",
    "#K_val = range(1, 20)\n",
    "#DisFuncs = {\"Dis1\": Dis1, \"Dis2\": Dis2, \"DisInf\": DisInf, \"DisCosine\": DisCosine}\n",
    "#\n",
    "#results = OrderedDict()\n",
    "#\n",
    "#for pfname, ParseFunc in ParseFuncs.items():\n",
    "#    word_dict = OrderedDict()\n",
    "#    def classifyParseX(fdata: List[List]): return ParseFunc(fdata, word_dict)\n",
    "#    trainX, trainY = vectorizeData(trainX_data, trainY_data, classifyParseX, classifyParseY)\n",
    "#    vaildX, vaildY = vectorizeData(vaildX_data, vaildY_data, classifyParseX, classifyParseY)\n",
    "#    for K in K_val:\n",
    "#        for dfname, DisFunc in DisFuncs.items():\n",
    "#            print(\"ParseFunc = {}, K = {}, DisFunc = {}\".format(pfname, K, dfname))\n",
    "#            def knnFunc(trainX, trainY, vaildX): return KNN((trainX, trainY), vaildX, DisFunc, K, DisInvNormAvg)\n",
    "#            ret = do_classify(trainX, trainY, vaildX, vaildY, knnFunc)\n",
    "#            results[(pfname, K, dfname)] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoTrain(train_filen, vaild_filen, ReadFileFunc, ParseYFunc, TrainFunc):\n",
    "    print(\"Start training...\")\n",
    "    t = time()\n",
    "    trainX_data, trainY_data = ReadFileFunc(train_filen)\n",
    "    vaildX_data, vaildY_data = ReadFileFunc(vaild_filen)\n",
    "    \n",
    "    ParseFuncs = {\"OneHot\": getOneHot, \"TI-IDF\": KNN_getTFIDF}\n",
    "    K_val = range(8, 14)\n",
    "    DisFuncs = {\"Dis1\": Dis1, \"Dis2\": Dis2, \"DisInf\": DisInf, \"DisCosine\": DisCosine}\n",
    "    #DisFuncs = {\"DisCosine\": DisCosine}\n",
    "    results = OrderedDict()\n",
    "    \n",
    "    for pfname, ParseFunc in ParseFuncs.items():\n",
    "        word_dict = OrderedDict()\n",
    "        def ParseXFunc(fdata: List[List]): return ParseFunc(fdata, word_dict)\n",
    "        trainX, trainY = vectorizeData(trainX_data, trainY_data, ParseXFunc, ParseYFunc)\n",
    "        vaildX, vaildY = vectorizeData(vaildX_data, vaildY_data, ParseXFunc, ParseYFunc)\n",
    "        for K in K_val:\n",
    "            for dfname, DisFunc in DisFuncs.items():\n",
    "                print(\"ParseFunc = {}, K = {}, DisFunc = {}\".format(pfname, K, dfname))\n",
    "                def knnFunc(trainX, trainY, vaildX): return KNN((trainX, trainY), vaildX, DisFunc, K, DisInvNormAvg)\n",
    "                ret = TrainFunc(trainX, trainY, vaildX, vaildY, knnFunc)\n",
    "                results[(pfname, K, dfname)] = ret\n",
    "    print(\"{} groups of argument tested, spent {}s\".format(len(ParseFuncs) * len(K_val) * len(DisFuncs), time() - t))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "ParseFunc = OneHot, K = 8, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.2598822818387406\n",
      "ParseFunc = OneHot, K = 8, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.24841844862532894\n",
      "ParseFunc = OneHot, K = 8, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.1384238210551513\n",
      "ParseFunc = OneHot, K = 8, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.3509801140585453\n",
      "ParseFunc = OneHot, K = 9, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.2574921244734181\n",
      "ParseFunc = OneHot, K = 9, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.24619015576418102\n",
      "ParseFunc = OneHot, K = 9, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.14026498579182584\n",
      "ParseFunc = OneHot, K = 9, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.35723121105176353\n",
      "ParseFunc = OneHot, K = 10, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.2629518663576531\n",
      "ParseFunc = OneHot, K = 10, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.25118330620673873\n",
      "ParseFunc = OneHot, K = 10, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.14204846176177952\n",
      "ParseFunc = OneHot, K = 10, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.34626546659053203\n",
      "ParseFunc = OneHot, K = 11, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.2735819005847752\n",
      "ParseFunc = OneHot, K = 11, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.2620526749094993\n",
      "ParseFunc = OneHot, K = 11, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.14275895429998073\n",
      "ParseFunc = OneHot, K = 11, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.32833459241003454\n",
      "ParseFunc = OneHot, K = 12, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.26976977476136266\n",
      "ParseFunc = OneHot, K = 12, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.25813325027104367\n",
      "ParseFunc = OneHot, K = 12, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.14165180080953435\n",
      "ParseFunc = OneHot, K = 12, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.32312340116875865\n",
      "ParseFunc = OneHot, K = 13, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.2697458961065652\n",
      "ParseFunc = OneHot, K = 13, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.2586560356721576\n",
      "ParseFunc = OneHot, K = 13, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.1435447273995338\n",
      "ParseFunc = OneHot, K = 13, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.32112232133753077\n",
      "ParseFunc = TI-IDF, K = 8, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.3136233628472089\n",
      "ParseFunc = TI-IDF, K = 8, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.24043610159869797\n",
      "ParseFunc = TI-IDF, K = 8, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.14964635554302866\n",
      "ParseFunc = TI-IDF, K = 8, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.4053642423346923\n",
      "ParseFunc = TI-IDF, K = 9, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.3146291925970572\n",
      "ParseFunc = TI-IDF, K = 9, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.2319286928256669\n",
      "ParseFunc = TI-IDF, K = 9, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.13992992473660174\n",
      "ParseFunc = TI-IDF, K = 9, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.4070312355638696\n",
      "ParseFunc = TI-IDF, K = 10, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.3041146357566817\n",
      "ParseFunc = TI-IDF, K = 10, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.24365770515480992\n",
      "ParseFunc = TI-IDF, K = 10, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.14523895953017393\n",
      "ParseFunc = TI-IDF, K = 10, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.4052144071304416\n",
      "ParseFunc = TI-IDF, K = 11, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.3111875963025045\n",
      "ParseFunc = TI-IDF, K = 11, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.24692525116607955\n",
      "ParseFunc = TI-IDF, K = 11, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.1411222999684326\n",
      "ParseFunc = TI-IDF, K = 11, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.3982698654500274\n",
      "ParseFunc = TI-IDF, K = 12, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.3163261098155004\n",
      "ParseFunc = TI-IDF, K = 12, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.2488297895109346\n",
      "ParseFunc = TI-IDF, K = 12, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.13259389813456346\n",
      "ParseFunc = TI-IDF, K = 12, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.394279061903327\n",
      "ParseFunc = TI-IDF, K = 13, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.3281013421794737\n",
      "ParseFunc = TI-IDF, K = 13, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.24797019847155602\n",
      "ParseFunc = TI-IDF, K = 13, DisFunc = DisInf\n",
      "Correlation Coefficient:  0.12864147476960483\n",
      "ParseFunc = TI-IDF, K = 13, DisFunc = DisCosine\n",
      "Correlation Coefficient:  0.38745096442658084\n",
      "48 groups of argument tested, spent 113.40418601036072s\n"
     ]
    }
   ],
   "source": [
    "train_filen = 'lab1_data/regression_dataset/train_set.csv'\n",
    "vaild_filen = 'lab1_data/regression_dataset/validation_set.csv'\n",
    "test_filen  = 'lab1_data/regression_dataset/test_set.csv'\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "regressResults = autoTrain(train_filen, vaild_filen, regressReadFile, regressParseY, do_regress)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "ParseFunc = OneHot, K = 8, DisFunc = Dis1\n",
      "Classification Accuracy:  0.3504823151125402\n",
      "ParseFunc = OneHot, K = 8, DisFunc = Dis2\n",
      "Classification Accuracy:  0.3440514469453376\n",
      "ParseFunc = OneHot, K = 8, DisFunc = DisInf\n",
      "Classification Accuracy:  0.3729903536977492\n",
      "ParseFunc = OneHot, K = 8, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.44694533762057875\n",
      "ParseFunc = OneHot, K = 9, DisFunc = Dis1\n",
      "Classification Accuracy:  0.38263665594855306\n",
      "ParseFunc = OneHot, K = 9, DisFunc = Dis2\n",
      "Classification Accuracy:  0.37942122186495175\n",
      "ParseFunc = OneHot, K = 9, DisFunc = DisInf\n",
      "Classification Accuracy:  0.3729903536977492\n",
      "ParseFunc = OneHot, K = 9, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4437299035369775\n",
      "ParseFunc = OneHot, K = 10, DisFunc = Dis1\n",
      "Classification Accuracy:  0.3954983922829582\n",
      "ParseFunc = OneHot, K = 10, DisFunc = Dis2\n",
      "Classification Accuracy:  0.3890675241157556\n",
      "ParseFunc = OneHot, K = 10, DisFunc = DisInf\n",
      "Classification Accuracy:  0.3729903536977492\n",
      "ParseFunc = OneHot, K = 10, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4437299035369775\n",
      "ParseFunc = OneHot, K = 11, DisFunc = Dis1\n",
      "Classification Accuracy:  0.4180064308681672\n",
      "ParseFunc = OneHot, K = 11, DisFunc = Dis2\n",
      "Classification Accuracy:  0.40836012861736337\n",
      "ParseFunc = OneHot, K = 11, DisFunc = DisInf\n",
      "Classification Accuracy:  0.3729903536977492\n",
      "ParseFunc = OneHot, K = 11, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4405144694533762\n",
      "ParseFunc = OneHot, K = 12, DisFunc = Dis1\n",
      "Classification Accuracy:  0.4180064308681672\n",
      "ParseFunc = OneHot, K = 12, DisFunc = Dis2\n",
      "Classification Accuracy:  0.40836012861736337\n",
      "ParseFunc = OneHot, K = 12, DisFunc = DisInf\n",
      "Classification Accuracy:  0.3729903536977492\n",
      "ParseFunc = OneHot, K = 12, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4405144694533762\n",
      "ParseFunc = OneHot, K = 13, DisFunc = Dis1\n",
      "Classification Accuracy:  0.43086816720257237\n",
      "ParseFunc = OneHot, K = 13, DisFunc = Dis2\n",
      "Classification Accuracy:  0.4212218649517685\n",
      "ParseFunc = OneHot, K = 13, DisFunc = DisInf\n",
      "Classification Accuracy:  0.3729903536977492\n",
      "ParseFunc = OneHot, K = 13, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.43729903536977494\n",
      "ParseFunc = TI-IDF, K = 8, DisFunc = Dis1\n",
      "Classification Accuracy:  0.3954983922829582\n",
      "ParseFunc = TI-IDF, K = 8, DisFunc = Dis2\n",
      "Classification Accuracy:  0.2057877813504823\n",
      "ParseFunc = TI-IDF, K = 8, DisFunc = DisInf\n",
      "Classification Accuracy:  0.36977491961414793\n",
      "ParseFunc = TI-IDF, K = 8, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4630225080385852\n",
      "ParseFunc = TI-IDF, K = 9, DisFunc = Dis1\n",
      "Classification Accuracy:  0.3987138263665595\n",
      "ParseFunc = TI-IDF, K = 9, DisFunc = Dis2\n",
      "Classification Accuracy:  0.20257234726688103\n",
      "ParseFunc = TI-IDF, K = 9, DisFunc = DisInf\n",
      "Classification Accuracy:  0.39228295819935693\n",
      "ParseFunc = TI-IDF, K = 9, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4533762057877814\n",
      "ParseFunc = TI-IDF, K = 10, DisFunc = Dis1\n",
      "Classification Accuracy:  0.40192926045016075\n",
      "ParseFunc = TI-IDF, K = 10, DisFunc = Dis2\n",
      "Classification Accuracy:  0.19935691318327975\n",
      "ParseFunc = TI-IDF, K = 10, DisFunc = DisInf\n",
      "Classification Accuracy:  0.38263665594855306\n",
      "ParseFunc = TI-IDF, K = 10, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4855305466237942\n",
      "ParseFunc = TI-IDF, K = 11, DisFunc = Dis1\n",
      "Classification Accuracy:  0.3954983922829582\n",
      "ParseFunc = TI-IDF, K = 11, DisFunc = Dis2\n",
      "Classification Accuracy:  0.21221864951768488\n",
      "ParseFunc = TI-IDF, K = 11, DisFunc = DisInf\n",
      "Classification Accuracy:  0.39228295819935693\n",
      "ParseFunc = TI-IDF, K = 11, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4855305466237942\n",
      "ParseFunc = TI-IDF, K = 12, DisFunc = Dis1\n",
      "Classification Accuracy:  0.3987138263665595\n",
      "ParseFunc = TI-IDF, K = 12, DisFunc = Dis2\n",
      "Classification Accuracy:  0.21543408360128619\n",
      "ParseFunc = TI-IDF, K = 12, DisFunc = DisInf\n",
      "Classification Accuracy:  0.36977491961414793\n",
      "ParseFunc = TI-IDF, K = 12, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4694533762057878\n",
      "ParseFunc = TI-IDF, K = 13, DisFunc = Dis1\n",
      "Classification Accuracy:  0.40514469453376206\n",
      "ParseFunc = TI-IDF, K = 13, DisFunc = Dis2\n",
      "Classification Accuracy:  0.2090032154340836\n",
      "ParseFunc = TI-IDF, K = 13, DisFunc = DisInf\n",
      "Classification Accuracy:  0.3858520900321543\n",
      "ParseFunc = TI-IDF, K = 13, DisFunc = DisCosine\n",
      "Classification Accuracy:  0.4790996784565916\n",
      "48 groups of argument tested, spent 120.28540706634521s\n"
     ]
    }
   ],
   "source": [
    "train_filen = 'lab1_data/classification_dataset/train_set.csv'\n",
    "vaild_filen = 'lab1_data/classification_dataset/validation_set.csv'\n",
    "test_filen  = 'lab1_data/classification_dataset/test_set.csv'\n",
    "classifyResults = autoTrain(train_filen, vaild_filen, classifyReadFile, classifyParseY, do_classify)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('TI-IDF', 9, 'DisCosine'), 0.4070312355638696),\n",
       " (('TI-IDF', 8, 'DisCosine'), 0.4053642423346923),\n",
       " (('TI-IDF', 10, 'DisCosine'), 0.4052144071304416),\n",
       " (('TI-IDF', 11, 'DisCosine'), 0.3982698654500274),\n",
       " (('TI-IDF', 12, 'DisCosine'), 0.394279061903327),\n",
       " (('TI-IDF', 13, 'DisCosine'), 0.38745096442658084),\n",
       " (('OneHot', 9, 'DisCosine'), 0.35723121105176353),\n",
       " (('OneHot', 8, 'DisCosine'), 0.3509801140585453),\n",
       " (('OneHot', 10, 'DisCosine'), 0.34626546659053203),\n",
       " (('OneHot', 11, 'DisCosine'), 0.32833459241003454),\n",
       " (('TI-IDF', 13, 'Dis1'), 0.3281013421794737),\n",
       " (('OneHot', 12, 'DisCosine'), 0.32312340116875865),\n",
       " (('OneHot', 13, 'DisCosine'), 0.32112232133753077),\n",
       " (('TI-IDF', 12, 'Dis1'), 0.3163261098155004),\n",
       " (('TI-IDF', 9, 'Dis1'), 0.3146291925970572),\n",
       " (('TI-IDF', 8, 'Dis1'), 0.3136233628472089),\n",
       " (('TI-IDF', 11, 'Dis1'), 0.3111875963025045),\n",
       " (('TI-IDF', 10, 'Dis1'), 0.3041146357566817),\n",
       " (('OneHot', 11, 'Dis1'), 0.2735819005847752),\n",
       " (('OneHot', 12, 'Dis1'), 0.26976977476136266),\n",
       " (('OneHot', 13, 'Dis1'), 0.2697458961065652),\n",
       " (('OneHot', 10, 'Dis1'), 0.2629518663576531),\n",
       " (('OneHot', 11, 'Dis2'), 0.2620526749094993),\n",
       " (('OneHot', 8, 'Dis1'), 0.2598822818387406),\n",
       " (('OneHot', 13, 'Dis2'), 0.2586560356721576),\n",
       " (('OneHot', 12, 'Dis2'), 0.25813325027104367),\n",
       " (('OneHot', 9, 'Dis1'), 0.2574921244734181),\n",
       " (('OneHot', 10, 'Dis2'), 0.25118330620673873),\n",
       " (('TI-IDF', 12, 'Dis2'), 0.2488297895109346),\n",
       " (('OneHot', 8, 'Dis2'), 0.24841844862532894),\n",
       " (('TI-IDF', 13, 'Dis2'), 0.24797019847155602),\n",
       " (('TI-IDF', 11, 'Dis2'), 0.24692525116607955),\n",
       " (('OneHot', 9, 'Dis2'), 0.24619015576418102),\n",
       " (('TI-IDF', 10, 'Dis2'), 0.24365770515480992),\n",
       " (('TI-IDF', 8, 'Dis2'), 0.24043610159869797),\n",
       " (('TI-IDF', 9, 'Dis2'), 0.2319286928256669),\n",
       " (('TI-IDF', 8, 'DisInf'), 0.14964635554302866),\n",
       " (('TI-IDF', 10, 'DisInf'), 0.14523895953017393),\n",
       " (('OneHot', 13, 'DisInf'), 0.1435447273995338),\n",
       " (('OneHot', 11, 'DisInf'), 0.14275895429998073),\n",
       " (('OneHot', 10, 'DisInf'), 0.14204846176177952),\n",
       " (('OneHot', 12, 'DisInf'), 0.14165180080953435),\n",
       " (('TI-IDF', 11, 'DisInf'), 0.1411222999684326),\n",
       " (('OneHot', 9, 'DisInf'), 0.14026498579182584),\n",
       " (('TI-IDF', 9, 'DisInf'), 0.13992992473660174),\n",
       " (('OneHot', 8, 'DisInf'), 0.1384238210551513),\n",
       " (('TI-IDF', 12, 'DisInf'), 0.13259389813456346),\n",
       " (('TI-IDF', 13, 'DisInf'), 0.12864147476960483)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(regressResults.items(), key=lambda kv: 0 if np.isnan(kv[1]) else kv[1] , reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('TI-IDF', 10, 'DisCosine'), 0.4855305466237942),\n",
       " (('TI-IDF', 11, 'DisCosine'), 0.4855305466237942),\n",
       " (('TI-IDF', 13, 'DisCosine'), 0.4790996784565916),\n",
       " (('TI-IDF', 12, 'DisCosine'), 0.4694533762057878),\n",
       " (('TI-IDF', 8, 'DisCosine'), 0.4630225080385852),\n",
       " (('TI-IDF', 9, 'DisCosine'), 0.4533762057877814),\n",
       " (('OneHot', 8, 'DisCosine'), 0.44694533762057875),\n",
       " (('OneHot', 9, 'DisCosine'), 0.4437299035369775),\n",
       " (('OneHot', 10, 'DisCosine'), 0.4437299035369775),\n",
       " (('OneHot', 11, 'DisCosine'), 0.4405144694533762),\n",
       " (('OneHot', 12, 'DisCosine'), 0.4405144694533762),\n",
       " (('OneHot', 13, 'DisCosine'), 0.43729903536977494),\n",
       " (('OneHot', 13, 'Dis1'), 0.43086816720257237),\n",
       " (('OneHot', 13, 'Dis2'), 0.4212218649517685),\n",
       " (('OneHot', 11, 'Dis1'), 0.4180064308681672),\n",
       " (('OneHot', 12, 'Dis1'), 0.4180064308681672),\n",
       " (('OneHot', 11, 'Dis2'), 0.40836012861736337),\n",
       " (('OneHot', 12, 'Dis2'), 0.40836012861736337),\n",
       " (('TI-IDF', 13, 'Dis1'), 0.40514469453376206),\n",
       " (('TI-IDF', 10, 'Dis1'), 0.40192926045016075),\n",
       " (('TI-IDF', 9, 'Dis1'), 0.3987138263665595),\n",
       " (('TI-IDF', 12, 'Dis1'), 0.3987138263665595),\n",
       " (('OneHot', 10, 'Dis1'), 0.3954983922829582),\n",
       " (('TI-IDF', 8, 'Dis1'), 0.3954983922829582),\n",
       " (('TI-IDF', 11, 'Dis1'), 0.3954983922829582),\n",
       " (('TI-IDF', 9, 'DisInf'), 0.39228295819935693),\n",
       " (('TI-IDF', 11, 'DisInf'), 0.39228295819935693),\n",
       " (('OneHot', 10, 'Dis2'), 0.3890675241157556),\n",
       " (('TI-IDF', 13, 'DisInf'), 0.3858520900321543),\n",
       " (('OneHot', 9, 'Dis1'), 0.38263665594855306),\n",
       " (('TI-IDF', 10, 'DisInf'), 0.38263665594855306),\n",
       " (('OneHot', 9, 'Dis2'), 0.37942122186495175),\n",
       " (('OneHot', 8, 'DisInf'), 0.3729903536977492),\n",
       " (('OneHot', 9, 'DisInf'), 0.3729903536977492),\n",
       " (('OneHot', 10, 'DisInf'), 0.3729903536977492),\n",
       " (('OneHot', 11, 'DisInf'), 0.3729903536977492),\n",
       " (('OneHot', 12, 'DisInf'), 0.3729903536977492),\n",
       " (('OneHot', 13, 'DisInf'), 0.3729903536977492),\n",
       " (('TI-IDF', 8, 'DisInf'), 0.36977491961414793),\n",
       " (('TI-IDF', 12, 'DisInf'), 0.36977491961414793),\n",
       " (('OneHot', 8, 'Dis1'), 0.3504823151125402),\n",
       " (('OneHot', 8, 'Dis2'), 0.3440514469453376),\n",
       " (('TI-IDF', 12, 'Dis2'), 0.21543408360128619),\n",
       " (('TI-IDF', 11, 'Dis2'), 0.21221864951768488),\n",
       " (('TI-IDF', 13, 'Dis2'), 0.2090032154340836),\n",
       " (('TI-IDF', 8, 'Dis2'), 0.2057877813504823),\n",
       " (('TI-IDF', 9, 'Dis2'), 0.20257234726688103),\n",
       " (('TI-IDF', 10, 'Dis2'), 0.19935691318327975)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(classifyResults.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4523855ebff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvaildX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParseXFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParseYFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknnFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisInvNormAvg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknnFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-0d95a243a6de>\u001b[0m in \u001b[0;36mdo_regress\u001b[0;34m(trainX, trainY, vaildX, vaildY, knnFunc, save)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"regress.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%4f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correlation Coefficient: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-0d95a243a6de>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"regress.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%4f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correlation Coefficient: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mpearsonr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   2997\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m     \u001b[0mmx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2999\u001b[0;31m     \u001b[0mmy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3000\u001b[0m     \u001b[0mxm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3001\u001b[0m     \u001b[0mr_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "train_filen = 'lab1_data/regression_dataset/train_set.csv'\n",
    "vaild_filen = 'lab1_data/regression_dataset/validation_set.csv'\n",
    "test_filen  = 'lab1_data/regression_dataset/test_set.csv'\n",
    "\n",
    "ReadFileFunc = regressReadFile\n",
    "ParseYFunc = regressParseY\n",
    "TrainFunc = do_regress\n",
    "\n",
    "ParseFunc = KNN_getTFIDF\n",
    "DisFunc = DisCosine\n",
    "K = 9\n",
    "\n",
    "trainX_data, trainY_data = ReadFileFunc(train_filen)\n",
    "vaildX_data, vaildY_data = ReadFileFunc(test_filen)\n",
    "word_dict = OrderedDict()\n",
    "def ParseXFunc(fdata: List[List]): return ParseFunc(fdata, word_dict)\n",
    "trainX, trainY = vectorizeData(trainX_data, trainY_data, ParseXFunc, ParseYFunc)\n",
    "vaildX, vaildY = vectorizeData(vaildX_data, vaildY_data, ParseXFunc, ParseYFunc)\n",
    "def knnFunc(trainX, trainY, vaildX): return KNN((trainX, trainY), vaildX, DisFunc, K, DisInvNormAvg)\n",
    "ret = TrainFunc(trainX, trainY, vaildX, vaildY, knnFunc, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
