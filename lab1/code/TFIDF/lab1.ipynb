{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from typing import Callable\n",
    "from typing import Any\n",
    "from typing import Dict, Tuple, List\n",
    "import cProfile\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filen: str) -> List[List]:\n",
    "    '''\n",
    "    读取文件内容\n",
    "    由于首先需要获取文章数量和单词向量长度，才能计算TF矩阵\n",
    "    因此要对文本内容进行两次遍历，为了避免两次读取磁盘文件，故先将文本内容保存到内存中的一个list\n",
    "    '''\n",
    "    fdata = []\n",
    "    with open(filen) as fd:\n",
    "        reader = csv.reader(fd, delimiter=' ')\n",
    "        fdata = [list(row) for row in reader]\n",
    "    return fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(fdata: List[List]) -> np.array:\n",
    "    '''\n",
    "    获取TF-IDF矩阵\n",
    "    '''\n",
    "    #首先获取文章数和单词向量\n",
    "    #使用OrderedDict按单词出现的顺序生成单词列表\n",
    "    #相比于使用list，好处在于每次判断word是否已经加入单词向量是log(n)复杂度\n",
    "    word_dict = OrderedDict() \n",
    "    #文章数\n",
    "    D = 0\n",
    "    for row in fdata:\n",
    "        D += 1\n",
    "        for word in row:\n",
    "            if not word in word_dict:\n",
    "                word_dict[word] = 1\n",
    "            else:\n",
    "                word_dict[word] += 1\n",
    "    #word_vec是单词向量\n",
    "    word_vec = word_dict.keys()\n",
    "    #word_order的键值是当前单词的序号，在生成TF矩阵时会用到\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    #生成TF矩阵\n",
    "    TF = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            TF[i][word_order[word]] += 1\n",
    "        #每个文章中单词出现次数归一化\n",
    "        TF[i] /= len(fdata[i])\n",
    "    #生成IDF矩阵\n",
    "    IDF = np.log(D / (1 + np.array(list(word_dict.values()))))\n",
    "\n",
    "    #生成TF-IDF矩阵\n",
    "    TF_IDF = np.multiply(TF, IDF)\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.20273255,  0.        ],\n",
       "       [ 0.        , -0.20273255,  0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTFIDF([[\"b\", \"c\"], [\"a\", \"c\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n"
     ]
    }
   ],
   "source": [
    "semval = readFile('lab1_data/semeval_sliced.txt')\n",
    "ret = getTFIDF(semval)\n",
    "#ret = ret.tolist()\n",
    "#rt = []\n",
    "#for row in ret:\n",
    "#    rr = []\n",
    "#    for w in row:\n",
    "#        if row != 0:\n",
    "#           rr.append(w)\n",
    "#    rt.append(rr)\n",
    "#ret = np.array(rt)\n",
    "##print(ret)\n",
    "np.savetxt(\"15323032_LiXinrui_TFIDF.txt\", ret, delimiter=\" \", fmt=\"%6f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
