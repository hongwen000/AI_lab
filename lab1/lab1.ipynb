{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from typing import Callable\n",
    "from typing import Any\n",
    "from typing import Dict, Tuple, List\n",
    "import cProfile\n",
    "import re\n",
    "import os\n",
    "from pycallgraph import PyCallGraph\n",
    "from pycallgraph.output import GraphvizOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filen: str) -> List[List]:\n",
    "    '''\n",
    "    读取文件内容\n",
    "    由于首先需要获取文章数量和单词向量长度，才能计算TF矩阵\n",
    "    因此要对文本内容进行两次遍历，为了避免两次读取磁盘文件，故先将文本内容保存到内存中的一个list\n",
    "    '''\n",
    "    fdata = []\n",
    "    with open(filen) as fd:\n",
    "        reader = csv.reader(fd, delimiter=' ')\n",
    "        fdata = [list(row) for row in reader]\n",
    "    return fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(fdata: List[List]) -> np.array:\n",
    "    '''\n",
    "    获取TF-IDF矩阵\n",
    "    '''\n",
    "    #首先获取文章数和单词向量\n",
    "    #使用OrderedDict按单词出现的顺序生成单词列表\n",
    "    #相比于使用list，好处在于每次判断word是否已经加入单词向量是log(n)复杂度\n",
    "    word_dict = OrderedDict() \n",
    "    #文章数\n",
    "    D = 0\n",
    "    for row in fdata:\n",
    "        D += 1\n",
    "        for word in row:\n",
    "            if not word in word_dict:\n",
    "                word_dict[word] = 1\n",
    "            else:\n",
    "                word_dict[word] += 1\n",
    "    #word_vec是单词向量\n",
    "    word_vec = word_dict.keys()\n",
    "    #word_order的键值是当前单词的序号，在生成TF矩阵时会用到\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    #生成TF矩阵\n",
    "    TF = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            TF[i][word_order[word]] += 1\n",
    "        #每个文章中单词出现次数归一化\n",
    "        TF[i] /= len(fdata[i])\n",
    "    #生成IDF矩阵\n",
    "    IDF = np.log2(D / (1 + np.array(list(word_dict.values()))))\n",
    "    #生成TF-IDF矩阵\n",
    "    TF_IDF = np.multiply(TF, IDF)\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06311440467834473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TF_IDF\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_getTFIDF(fdata: List[List], word_dict: OrderedDict) -> np.array:\n",
    "    '''\n",
    "    获取TF-IDF矩阵，并将每个单词及出现次数存储到word_dict中\n",
    "    '''\n",
    "    #首先获取文章数和单词向量\n",
    "    #使用OrderedDict按单词出现的顺序生成单词列表\n",
    "    #相比于使用list，好处在于每次判断word是否已经加入单词向量是log(n)复杂度\n",
    "    #文章数\n",
    "    D = len(fdata)\n",
    "    if len(word_dict) is 0:\n",
    "        #训练集\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if not word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "                else:\n",
    "                    word_dict[word] += 1\n",
    "        word_dict[None] = 0\n",
    "    else:\n",
    "        #验证集和测试集，丢弃未出现的单词\n",
    "        word_dict = dict(zip(word_dict.keys(), [0 for _ in word_dict.values()]))\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word] += 1\n",
    "                else:\n",
    "                    word_dict[None] += 1\n",
    "    #word_vec是单词向量\n",
    "    word_vec = word_dict.keys()\n",
    "    #word_order的键值是当前单词的序号，在生成TF矩阵时会用到\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    #生成TF矩阵\n",
    "    TF = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            if word in word_order:\n",
    "                TF[i][word_order[word]] += 1\n",
    "            else:\n",
    "                TF[i][word_order[None]] += 1\n",
    "        #每个文章中单词出现次数归一化\n",
    "        TF[i] /= len(fdata[i])\n",
    "    #生成IDF矩阵\n",
    "    IDF = np.log2(D / (1 + np.array(list(word_dict.values()))))\n",
    "    #生成TF-IDF矩阵\n",
    "    TF_IDF = np.multiply(TF, IDF)\n",
    "    np.savetxt(\"foo.csv\", TF_IDF, delimiter=\",\")\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. ,  0. ,  0. ],\n",
       "       [-0.5,  0. ,  0. ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdict = OrderedDict()\n",
    "KNN_getTFIDF([['a','a'],['c','a']], wdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneHot(fdata: List[List], word_dict: OrderedDict) -> np.array:\n",
    "    D = len(fdata)\n",
    "    if len(word_dict) is 0:\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if not word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "        word_dict[None] = 0\n",
    "    else:\n",
    "        word_dict = dict(zip(word_dict.keys(), [0 for _ in word_dict.values()]))\n",
    "        for row in fdata:\n",
    "            for word in row:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word] = 1\n",
    "                else:\n",
    "                    word_dict[None] = 1\n",
    "    word_vec = word_dict.keys()\n",
    "    word_order = dict(zip(word_vec,range(len(word_vec))))\n",
    "    oneHot = np.zeros((D,len(word_dict)))\n",
    "    for i,row in enumerate(fdata):\n",
    "        for word in row:\n",
    "            if word in word_order:\n",
    "                oneHot[i][word_order[word]] = 1\n",
    "            else:\n",
    "                oneHot[i][word_order[None]] = 1\n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdict = OrderedDict()\n",
    "getOneHot([['a','a'],['c','a']], wdict)\n",
    "getOneHot([['d','d']], wdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisN(vec1: np.array, vec2: np.array, N: Any) -> float:\n",
    "    '''\n",
    "    计算N-norm\n",
    "    '''\n",
    "    if(N < 1):\n",
    "        raise ValueError(\"norm should be a positive integer or np.inf\")\n",
    "    if np.isinf(N):\n",
    "        return np.max(np.fabs(vec1 - vec2))\n",
    "    else:\n",
    "        return np.power(np.sum(np.power(vec1 - vec2, N)), 1.0/N)\n",
    "\n",
    "#Dis2 = lambda v1, v2: DisN(v1, v2, 2)\n",
    "Dis1 = lambda v1, v2: np.linalg.norm(v1 - v2, 1)\n",
    "Dis2 = lambda v1, v2: np.linalg.norm(v1 - v2, 2)\n",
    "DisInf = lambda v1, v2: np.linalg.norm(v1 - v2, np.inf)\n",
    "def DisCosine(v1, v2):\n",
    "    t1 = np.dot(v1,v2)\n",
    "    t2 = np.linalg.norm(v1)\n",
    "    t3 = np.linalg.norm(v2)\n",
    "    ret = 1 - t1 / (t2*t3)\n",
    "    return ret\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisCosine([0,1],[0,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisInvNormAvg(distances: np.array, Y: np.array) -> np.array:\n",
    "    '''\n",
    "    将距离倒数归一化，返回均值\n",
    "    '''\n",
    "    for idx, dis in enumerate(distances):\n",
    "        if np.isclose(dis, 0):\n",
    "            return Y[idx]\n",
    "    distances = np.array(1.0) / distances\n",
    "    s = np.sum(distances)\n",
    "    distances = distances / s\n",
    "    tmp = np.diag(distances) @ Y  \n",
    "    if len(tmp.shape) is 1:\n",
    "        return tmp\n",
    "    else:\n",
    "        return np.sum(tmp,  axis = (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(trainSet: Tuple[np.array, np.array],\n",
    "        testVec: np.array,\n",
    "        DisFunc: Callable[[np.array, np.array], float],\n",
    "        K: int,\n",
    "        WeightFunc: Callable[[np.array, np.array], float]) -> np.array: \n",
    "    '''\n",
    "    一个通用的KNN接口\n",
    "    trainSet: 二元元组，第一个元素是训练集的X，第二个是Y\n",
    "    testVec: 待预测向量\n",
    "    DisFunc: 距离函数\n",
    "    K: K值\n",
    "    WeightFunc: 依据第一个参数list<距离>,对第二个参数list<Y值>进行加权，返回预测值\n",
    "    '''\n",
    "    #对于多个要预测的值，逐一预测\n",
    "    if len(testVec.shape) > 1:\n",
    "        return np.array([KNN(trainSet, vec, DisFunc, K, WeightFunc) for vec in testVec])\n",
    "    else:\n",
    "        #测量待预测向量到训练集中每个向量的距离\n",
    "        #distances是一个list<tuple(index, distance)>\n",
    "        distances = list(enumerate(map(lambda trainVec: DisFunc(trainVec, testVec), trainSet[0])))\n",
    "        #依据距离从小到大排序\n",
    "        distances.sort(key=lambda t: t[1])\n",
    "        #获取最临近的K个训练样本的下标和对应的距离，输出值\n",
    "        tmp = list(zip(*distances[:K]))\n",
    "        kNearIdx = list(tmp[0])\n",
    "        kNearDis = list(tmp[1])\n",
    "        kNearY   = trainSet[1][kNearIdx, :]\n",
    "        #对输出值根据距离加权作为预测输出\n",
    "        return WeightFunc(kNearDis, kNearY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestCast1():\n",
    "    trainX = np.array([[10,2],[2,3],[3,5]])\n",
    "    trainY = np.array([[1,1,1], [2,2,3], [3,3,5]])\n",
    "    vaildX = np.array([[3,3]])\n",
    "    return trainX, trainY, vaildX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestCast2():\n",
    "    xfilen = 'lab1_data/X.txt'\n",
    "    yfilen = 'lab1_data/Y.txt'\n",
    "    xdata = readFile(xfilen)\n",
    "    ydata = readFile(yfilen)\n",
    "    x_set = getTFIDF(xdata)\n",
    "    y_set = np.array([list(map(float, row)) for row in ydata])\n",
    "    DIVIDE_RATE = 0.75\n",
    "    train_D = int(np.ceil(x_set.shape[0] * DIVIDE_RATE))\n",
    "    trainX = x_set[0:train_D, :]\n",
    "    vaildX = x_set[train_D:, :]\n",
    "    trainY = y_set[0:train_D, :]\n",
    "    vaildY = y_set[train_D:, :]\n",
    "    return trainX, trainY, vaildX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(trainX, trainY, vaildX):\n",
    "    t = time()\n",
    "    cProfile.run('KNN((trainX, trainY), vaildX,Dis2,2,DisInvNormAvg)')\n",
    "    print(time()-t) \n",
    "#trainX, trainY, vaildX = TestCast2()\n",
    "#Test(trainX, trainY, vaildX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyReadFile(filen: str) -> Tuple[List[str], List[str]]:\n",
    "    with open(filen) as fd:\n",
    "        reader = csv.reader(fd, delimiter=',')\n",
    "        train_data = [list(row) for row in reader]\n",
    "        train_data = train_data[1:]\n",
    "        tmp = list(zip(*train_data))\n",
    "        xdata = [row.split() for row in list(tmp[0])]\n",
    "        ydata = list(tmp[1])\n",
    "        return xdata, ydata\n",
    "\n",
    "def vectorizeData(xdata, ydata, xVecFunc, yVecFunc):\n",
    "    return xVecFunc(xdata), yVecFunc(ydata)\n",
    "\n",
    "def fastHashY(s: str) -> int:\n",
    "    if s[0] is 'a': return 0\n",
    "    if s[0] is 'd': return 1\n",
    "    if s[0] is 'f': return 2\n",
    "    if s[0] is 'j': return 3\n",
    "    if s[1] is 'a': return 4\n",
    "    return 5\n",
    "\n",
    "def classifyParseY(ydata: List[str])->np.array:\n",
    "    '''\n",
    "    Convert Y data from raw string list to matrix consisted of Y vectors\n",
    "    e.g.\n",
    "    [\"anger\", \"disgust\", ..., \"surprise\"] -> \n",
    "    |1, 0, 0, 0, 0, 0|\n",
    "    |0, 1, 0, 0, 0, 0|\n",
    "    |0, 0, ...,  0, 0|\n",
    "    |0, 0, 0, 0, 1, 0|\n",
    "    |0, 0, 0, 0, 0, 1|\n",
    "    '''\n",
    "    D = len(ydata)\n",
    "    \n",
    "    #fast hash ydata from strings [\"anger\", \"disgust\", ...] to [1, 2, ...]^T\n",
    "    ydata = np.array(list(map(fastHashY, ydata))).reshape((-1,1))\n",
    "    \n",
    "    '''\n",
    "    ymat is the column-wise repeat of ydata.\n",
    "    e.g.\n",
    "    |0|      |0, 0, 0, 0, 0, 0|\n",
    "    |1|   -> |1, 1, 1, 1, 1, 1|\n",
    "    ...      |................|\n",
    "    |5|      |5, 5, 5, 5, 5, 5|\n",
    "    ydata -> ymat\n",
    "    '''\n",
    "    ymat  = np.tile(ydata, (1, 6))\n",
    "    \n",
    "    '''\n",
    "    ycmp is a matrix of which each row is [0, 1, 2, 3, 4, 5]\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    |................|\n",
    "    |0, 1, 2, 3, 4, 5|\n",
    "    '''\n",
    "    ycmp  = np.tile(np.array(range(6)), (D, 1))\n",
    "    return np.int_(np.equal(ymat, ycmp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_classify(trainX, trainY, vaildX, vaildY, knnFunc):\n",
    "    predictY = knnFunc(trainX, trainY, vaildX)\n",
    "    classifyY = np.zeros_like(predictY)\n",
    "    for i, row in enumerate(predictY):\n",
    "        m = 0\n",
    "        idx = 0\n",
    "        for j, v in enumerate(row):\n",
    "            if v > m:\n",
    "                m = v\n",
    "                idx = j\n",
    "        classifyY[i][idx] = 1\n",
    "    #print(\"Predicted Y\")\n",
    "    #print(classifyY)\n",
    "    #print(\"Correct Y\")\n",
    "    #print(vaildY)\n",
    "    ret = np.sum(np.logical_and(classifyY, vaildY)) / vaildX.shape[0]\n",
    "    print(\"Classification Accuracy: \", ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressReadFile(filen: str) -> Tuple[List[str], List[str]]:\n",
    "    with open(filen) as fd:\n",
    "        reader = csv.reader(fd, delimiter=',')\n",
    "        train_data = [list(row) for row in reader]\n",
    "        train_data = train_data[1:]\n",
    "        xdata = [row[0].split() for row in train_data]\n",
    "        ydata = [[row[i] for i in range(1, 7)] for row in train_data]\n",
    "        return xdata, ydata\n",
    "\n",
    "def regressParseY(ydata: List[List[str]]) -> np.array:\n",
    "    return np.float_(np.array(ydata))\n",
    "\n",
    "#from scipy.stats.stats import pearsonr\n",
    "def pearsonr(X, Y):\n",
    "\n",
    "    X_bar = np.average(X)\n",
    "    Y_bar = np.average(Y)\n",
    "    X = np.subtract(X, X_bar)\n",
    "    Y = np.subtract(Y, Y_bar)\n",
    "    t1 = np.sum(np.dot(X, Y))\n",
    "    t2 = np.sum(np.power(X,2))\n",
    "    t3 = np.sum(np.power(Y,2))\n",
    "\n",
    "    ret = t1 / np.power(t2 * t3, 0.5)\n",
    "    return ret\n",
    "def do_regress(trainX, trainY, vaildX, vaildY, knnFunc, save = False):\n",
    "    predictY = knnFunc(trainX, trainY, vaildX)\n",
    "    if save:\n",
    "        np.savetxt(\"regress.csv\", predictY, delimiter=\",\", fmt=\"%4f\")\n",
    "    r = [pearsonr(predictY[:, i], vaildY[:, i]) for i in range(6)]\n",
    "    average = np.average(r)\n",
    "    print(\"Correlation Coefficient: \", average)\n",
    "    return average\n",
    "    \n",
    "    \n",
    "train_filen = 'lab1_data/regression_dataset/train_set.csv'\n",
    "vaild_filen = 'lab1_data/regression_dataset/validation_set.csv'\n",
    "test_filen  = 'lab1_data/regression_dataset/test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_filen = 'lab1_data/classification_dataset/train_set.csv'\n",
    "#vaild_filen = 'lab1_data/classification_dataset/validation_set.csv'\n",
    "#test_filen  = 'lab1_data/classification_dataset/test_set.csv'\n",
    "#\n",
    "#trainX_data, trainY_data = classifyReadFile(train_filen)\n",
    "#vaildX_data, vaildY_data = classifyReadFile(vaild_filen)\n",
    "#\n",
    "#ParseFuncs = {\"OneHot\": getOneHot, \"TI-IDF\": KNN_getTFIDF}\n",
    "#K_val = range(1, 20)\n",
    "#DisFuncs = {\"Dis1\": Dis1, \"Dis2\": Dis2, \"DisInf\": DisInf, \"DisCosine\": DisCosine}\n",
    "#\n",
    "#results = OrderedDict()\n",
    "#\n",
    "#for pfname, ParseFunc in ParseFuncs.items():\n",
    "#    word_dict = OrderedDict()\n",
    "#    def classifyParseX(fdata: List[List]): return ParseFunc(fdata, word_dict)\n",
    "#    trainX, trainY = vectorizeData(trainX_data, trainY_data, classifyParseX, classifyParseY)\n",
    "#    vaildX, vaildY = vectorizeData(vaildX_data, vaildY_data, classifyParseX, classifyParseY)\n",
    "#    for K in K_val:\n",
    "#        for dfname, DisFunc in DisFuncs.items():\n",
    "#            print(\"ParseFunc = {}, K = {}, DisFunc = {}\".format(pfname, K, dfname))\n",
    "#            def knnFunc(trainX, trainY, vaildX): return KNN((trainX, trainY), vaildX, DisFunc, K, DisInvNormAvg)\n",
    "#            ret = do_classify(trainX, trainY, vaildX, vaildY, knnFunc)\n",
    "#            results[(pfname, K, dfname)] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoTrain(train_filen, vaild_filen, ReadFileFunc, ParseYFunc, TrainFunc):\n",
    "    print(\"Start training...\")\n",
    "    t = time()\n",
    "    trainX_data, trainY_data = ReadFileFunc(train_filen)\n",
    "    vaildX_data, vaildY_data = ReadFileFunc(vaild_filen)\n",
    "    \n",
    "    ParseFuncs = {\"OneHot\": getOneHot, \"TI-IDF\": KNN_getTFIDF}\n",
    "    K_val = range(1, 20)\n",
    "    DisFuncs = {\"Dis1\": Dis1, \"Dis2\": Dis2, \"DisInf\": DisInf, \"DisCosine\": DisCosine}\n",
    "    #DisFuncs = {\"DisCosine\": DisCosine}\n",
    "    results = OrderedDict()\n",
    "    \n",
    "    for pfname, ParseFunc in ParseFuncs.items():\n",
    "        word_dict = OrderedDict()\n",
    "        def ParseXFunc(fdata: List[List]): return ParseFunc(fdata, word_dict)\n",
    "        trainX, trainY = vectorizeData(trainX_data, trainY_data, ParseXFunc, ParseYFunc)\n",
    "        vaildX, vaildY = vectorizeData(vaildX_data, vaildY_data, ParseXFunc, ParseYFunc)\n",
    "        for K in K_val:\n",
    "            for dfname, DisFunc in DisFuncs.items():\n",
    "                print(\"ParseFunc = {}, K = {}, DisFunc = {}\".format(pfname, K, dfname))\n",
    "                def knnFunc(trainX, trainY, vaildX): return KNN((trainX, trainY), vaildX, DisFunc, K, DisInvNormAvg)\n",
    "                ret = TrainFunc(trainX, trainY, vaildX, vaildY, knnFunc)\n",
    "                results[(pfname, K, dfname)] = ret\n",
    "    print(\"{} groups of argument tested, spent {}s\".format(len(ParseFuncs) * len(K_val) * len(DisFuncs), time() - t))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "ParseFunc = OneHot, K = 1, DisFunc = Dis1\n",
      "Correlation Coefficient:  0.19479721872198139\n",
      "ParseFunc = OneHot, K = 1, DisFunc = Dis2\n",
      "Correlation Coefficient:  0.19479721872198139\n",
      "ParseFunc = OneHot, K = 1, DisFunc = DisInf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d08bafda7922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_filen\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m'lab1_data/regression_dataset/test_set.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mregressResults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaild_filen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregressReadFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregressParseY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_regress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-46ce898a7d13>\u001b[0m in \u001b[0;36mautoTrain\u001b[0;34m(train_filen, vaild_filen, ReadFileFunc, ParseYFunc, TrainFunc)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ParseFunc = {}, K = {}, DisFunc = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mknnFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisInvNormAvg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknnFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} groups of argument tested, spent {}s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParseFuncs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDisFuncs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-8fe2e21e84ee>\u001b[0m in \u001b[0;36mdo_regress\u001b[0;34m(trainX, trainY, vaildX, vaildY, knnFunc, save)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdo_regress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknnFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpredictY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknnFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"regress.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%4f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-46ce898a7d13>\u001b[0m in \u001b[0;36mknnFunc\u001b[0;34m(trainX, trainY, vaildX)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisFunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDisFuncs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ParseFunc = {}, K = {}, DisFunc = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mdef\u001b[0m \u001b[0mknnFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisInvNormAvg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknnFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f315326d6337>\u001b[0m in \u001b[0;36mKNN\u001b[0;34m(trainSet, testVec, DisFunc, K, WeightFunc)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#对于多个要预测的值，逐一预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestVec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeightFunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestVec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#测量待预测向量到训练集中每个向量的距离\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f315326d6337>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#对于多个要预测的值，逐一预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestVec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDisFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeightFunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestVec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#测量待预测向量到训练集中每个向量的距离\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f315326d6337>\u001b[0m in \u001b[0;36mKNN\u001b[0;34m(trainSet, testVec, DisFunc, K, WeightFunc)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#测量待预测向量到训练集中每个向量的距离\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#distances是一个list<tuple(index, distance)>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrainVec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDisFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#依据距离从小到大排序\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f315326d6337>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trainVec)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#测量待预测向量到训练集中每个向量的距离\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#distances是一个list<tuple(index, distance)>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrainVec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDisFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#依据距离从小到大排序\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-dc14247fe705>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(v1, v2)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mDis1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mDis2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mDisInf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDisCosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mInf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2378\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mInf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m     26\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     27\u001b[0m           initial=_NoValue):\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_filen = 'lab1_data/regression_dataset/train_set.csv'\n",
    "vaild_filen = 'lab1_data/regression_dataset/validation_set.csv'\n",
    "test_filen  = 'lab1_data/regression_dataset/test_set.csv'\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "regressResults = autoTrain(train_filen, vaild_filen, regressReadFile, regressParseY, do_regress)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filen = 'lab1_data/classification_dataset/train_set.csv'\n",
    "vaild_filen = 'lab1_data/classification_dataset/validation_set.csv'\n",
    "test_filen  = 'lab1_data/classification_dataset/test_set.csv'\n",
    "classifyResults = autoTrain(train_filen, vaild_filen, classifyReadFile, classifyParseY, do_classify)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(regressResults.items(), key=lambda kv: 0 if np.isnan(kv[1]) else kv[1] , reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(classifyResults.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Coefficient:  0.40703123556386966\n"
     ]
    }
   ],
   "source": [
    "graphviz = GraphvizOutput(output_file='filter_none.png')\n",
    "with PyCallGraph(output=graphviz):\n",
    "\n",
    "    train_filen = 'lab1_data/regression_dataset/train_set.csv'\n",
    "    vaild_filen = 'lab1_data/regression_dataset/validation_set.csv'\n",
    "    test_filen  = 'lab1_data/regression_dataset/test_set.csv'\n",
    "\n",
    "    ReadFileFunc = regressReadFile\n",
    "    ParseYFunc = regressParseY\n",
    "    TrainFunc = do_regress\n",
    "\n",
    "    ParseFunc = KNN_getTFIDF\n",
    "    DisFunc = DisCosine\n",
    "    K = 9\n",
    "\n",
    "    trainX_data, trainY_data = ReadFileFunc(train_filen)\n",
    "    vaildX_data, vaildY_data = ReadFileFunc(vaild_filen)\n",
    "    word_dict = OrderedDict()\n",
    "    def ParseXFunc(fdata: List[List]): return ParseFunc(fdata, word_dict)\n",
    "    trainX, trainY = vectorizeData(trainX_data, trainY_data, ParseXFunc, ParseYFunc)\n",
    "    vaildX, vaildY = vectorizeData(vaildX_data, vaildY_data, ParseXFunc, ParseYFunc)\n",
    "    def knnFunc(trainX, trainY, vaildX): return KNN((trainX, trainY), vaildX, DisFunc, K, DisInvNormAvg)\n",
    "    ret = TrainFunc(trainX, trainY, vaildX, vaildY, knnFunc, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
