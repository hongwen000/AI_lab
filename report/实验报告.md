## 数据预处理

### 数据清理

作为原始的自然语言文本, 在对二分类和五分类的训练样本向量化之前，我进行了以下预处理操作：

1. 去除标点、特殊符号、HTML标签等非英文内容

   代码：

   ```python
   def remove_punc(s):
       # 使用正则表达式去除标点和特殊符号
       regex_cleaner = regex.compile("[^a-zA-Z0-9]")
       return regex_cleaner.sub(' ', s)
   ```

2. 进行拼写检查

   使用的是PyEnchant工具和aspell-en字典

   结果在5635182个单词中发现135529个拼写错误（错误率2.4%）

   发现它把很多人名识别为了错误，并把很多英国英语单词改为了美国英语单词。因此首先添加了是否首字母大写的判断，并改用了英国英语数据库。

   ![image-20181018225637692](../../onedrive/note/分布式/10-17.assets/image-20181018225637692.png)

3. 词形归一化（包括大写转小写）

4. 去除简单词

## Bagging

### 算法原理

对原数据集进行M次放回的采样

减少算法的过拟合

### 伪代码

```python
rets = 0
for i in range(M):
    # D是样本集
    sub_D = sample_D(D)
    # A是特征集
    tree = CART.train(sub_D, A)
    ret = tree.vaild(vaild_set)
    rets += ret
rets /= M
```

### 从Bagging到随机森林

CART的问题：是一个贪婪算法，即便使用Bagging，得到的树在结构上仍然相似，结果高度相关。

为了减少模型间的相关度，随机森林算法在特征集上也进行抽样

```python
rets = 0
for i in range(M):
    # D是样本集
    sub_D = sample_D(D)
    # A是特征集
    sub_A = sample_A(A)
    tree = CART.train(sub_D, sub_A)
    ret = tree.vaild(vaild_set)
    rets += ret
rets /= M
```

## 回归树

### 数据划分

将测试数据依据某个特征$a$，以$s$为切分点进行切分。求解
$$
argmin_{a,s}[\sum_{x_i<s}(avg_1 - y_i)^2 + \sum_{x_i\ge s}(avg_2 - y_i)^2]\\
其中avg_1是s左边数据的平均值\\
avg_2是s及右边数据的平均值
$$
每个区域的输出即是平均值

### 终止条件

1. 如果当前结点设为叶节点,误差函数的值差距小于阈值
   $$
   \sum_{i\in D}(avg - y_i)^2 < tolerance
   $$

2. 选择最好的分割方法,不纯度减少量仍低于阈值
   $$
   \sum_{x_i<s}(avg_1 - y_i)^2 + \sum_{x_i\ge s}(avg_2 - y_i)^2 - \sum_{i\in D}(avg - y_i)^2 < tolerance
   $$

3. 数据量小于阈值
   $$
   |D| < toleracnce
   $$





## KNN

并行加速

我尝试了使用