## Bagging

### 算法原理

对原数据集进行M次放回的采样

减少算法的过拟合

### 伪代码

```python
rets = 0
for i in range(M):
    # D是样本集
    sub_D = sample_D(D)
    # A是特征集
    tree = CART.train(sub_D, A)
    ret = tree.vaild(vaild_set)
    rets += ret
rets /= M
```

### 从Bagging到随机森林

CART的问题：是一个贪婪算法，即便使用Bagging，得到的树在结构上仍然相似，结果高度相关。

为了减少模型间的相关度，随机森林算法在特征集上也进行抽样

```python
rets = 0
for i in range(M):
    # D是样本集
    sub_D = sample_D(D)
    # A是特征集
    sub_A = sample_A(A)
    tree = CART.train(sub_D, sub_A)
    ret = tree.vaild(vaild_set)
    rets += ret
rets /= M
```

## 回归树

### 数据划分

将测试数据依据某个特征$a$，以$s$为切分点进行切分。求解
$$
argmin_{a,s}[\sum_{x_i<s}(avg_1 - y_i)^2 + \sum_{x_i\ge s}(avg_2 - y_i)^2]\\
其中avg_1是s左边数据的平均值\\
avg_2是s及右边数据的平均值
$$
每个区域的输出即是平均值

### 终止条件

1. 差距小于阈值
2. 深度达到阈值
3. 不纯度减少量低于阈值
4. 数据量小于阈值